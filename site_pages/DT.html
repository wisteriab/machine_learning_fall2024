<!DOCTYPE html>
<html>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
    <style>
        body {
            background-color: #EAE7DC ;
            margin: 0px;
            font-family:'Arial', 'Helvetica', sans-serif;
            font-size: 18px;
        }

        .home-image {
            background-image: url(./site_pages/IMG_3777.jpg);
            background-repeat: no-repeat;
            background-attachment: fixed;
            background-position: center top;
            background-size: 100%;
            height: 400px;
        }

        .home-image-text {
            left:1%;
            top:27%;
            position: relative;
            color: #e85a4f;
        }

        .navbar {
            overflow: hidden;
            background-color: #D8C3A5;
        }

        .navbar a {
            float: left;
            color: #e85a4f;
            text-align: center;
            font-size: 18 px;
            padding: 20px 24px;
            text-decoration: none;
        }

        .navbar a.active{
            background-color: #e98074;
            color: #EAE7DC;
        }

        .navbar a:hover{
            background-color: #8E8D8A;
            color: #EAE7DC;
        }

        p {
            text-indent: 50px;
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        a {
            text-decoration: none;
        }    

        h1 {
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        h3 {
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
        }

        .pdf {
        width: 100%;
        aspect-ratio: 4 / 3;
        }

    </style>
</head>
<body>
    <div class = "navbar">
        <a href ="../index.html">Introduction</a>
        <a href = "./EDA.html">EDA</a>
        <a href = "./ARM.html">ARM</a>
        <a href = './Clustering.html'>Clustering</a>
        <a href = "./NB.html">Naive Bayes</a>
        <a href = "./PCA.html">PCA</a>
        <a href = "./Regression.html">Regression</a>
        <a href = "./SVM.html">SVM</a>
        <a class = "active" href = "./DT.html">DT</a>
        <a href = "./Conclusions.html">Conclusions</a>
    </div>

    <h1>Decision Trees</h1>
    <h3>Introduction</h3>
    <p>
        Decision Trees are a method of partitioning data in supervised learning.
        Through this a top down tree is designed in such a way that each parent node splits the data into 2 or more groups besed on the data itself.
        For example the data could split based on categories or certain values (for continuous or discrete data) which feeds into another level of the tree.
        Each location where a split occures is called a node and the split above it is called a parent node.
        The original split/node is referred to as the root node.
        Each set of sibling nodes exist within a given level of the tree. 
        Lastly, the height of the tree is referred to as the depth.
        Decision trees are particularly good at visualizing data for easy and quick interpretation of how the data feeds into the target variables and their respective classifications.
        The ultimate goal in the decision trees is to see how partitioning the data leads to the target variables being classified as they are.
        One common example of decision trees is medical practitioners.
        They often use decision trees to classify illness by asking more specific, leading questions.
        One could start with height, followed by weight, followed by symptoms, and lastly recent exposures to ultimately guide the doctor to the proper diagnosis.
        Another example of a decision tree is displayed below for ease of understanding (referenced from <a href = "https://towardsdatascience.com/decision-tree-hugging-b8851f853486">TowardsDataScience.com</a>).
    </p>
    <h3 class = "center">A Simple Decision Tree</h3>
    <img src = "./plots/decision_tree_simple.png" class = "center">
    <p>
        Decision tree efficiency and efficacy is often viewed through three specific measurements: the Gini index, information gain, and entropy.
        Entropy is the measure of randomness, uncertainty, or imputiry within the dataset itself.
        E.g. a measure of entropy of 0 means the data is perfectly pure (or homogenous) while a measurement of 1 means that the dataset is perfectly impure.
        As one can infer from the above clarification, entropy is bounded between zero and one, and it is calculatied by taking the sum of the probability times the log (base 2) of the probability of a datapoint existing within a given category for all categories.
        Information gain is an extension of entropy and enables data analysts and data scientists to understand how a node in a decision tree performs by the amount of entropy that is lost.
        This is done by measuring the entropy both before and after a node is implemented.
        The last measurement used in the efficacy of decision trees in the gini index, and is the most complex of the three.
        The gini index is another way to measure randomness of the dataset but is used specifically from a probabilistic standpoint.
        Said index is also a measurement between 0 and 1 and is the measurment of the likelihood of a feature being classified wrong at a specific point.
        When the index is 0, then the node is determined to be pure and the probability of misclassification is 0.
        All three of these are displayed below.
    </p>
    <h3 class = "center">Entropy Equation</h3>
    <img src = "./plots/Entropy.png" class = "center">
    <p class = "center">Referenced from <a href = "https://towardsdatascience.com/decision-tree-part-2-34b31b1dc328">here</a></p>
    <h3 class = "center">Gini Index Equation</h3>
    <img src = "./plots/gini_index.png" class = "center">
    <p class = "center">Referenced from <a href = "https://stats.stackexchange.com/questions/477063/how-to-derive-equation-of-gini-index-used-in-decision-trees">here</a></p>
    <p>
        Using all three of these in tandem with one another is paramount to maximizing the efficiency of the decision tree.
        More precisely, by maximizing both the information gain and reducing the gini index over time enables one to select the correct partition to use in a given node.
        Because of this, there is a particular solution that is the most efficient in classification based on outcomes (and many that can be similarly helpful).
        As such the goal of a decision tree is to reduce the entropy in the data set by maximizing the information gain in each level of the tree while minimizing either the entropy OR the gini index.
        Without using both information gain and one other metric it becomes near impossible to maximize the efficacy of the nodes, ultimately making the tree less "greedy."
        However, because it is possible to set the splits in the data at any point or value it is possible to create any number of trees that is equal to the number of data points by extending the depth or complexity of the tree.
        It is also technically possible to create an infinite number of trees (by using ineffective data) in the case that the data is not disjoint and is circular leading to trees that feed into themselves and continue to split indefinitely.
        This is explained futher below.
    </p>
    <h3>Data Cleaning and Prep</h3>
    <p>
        In order to create a decision tree, specific types of data must be input into the system.
        The data inputs themselves can be both numerical and categorical, however the dataset as a whole must be <i>disjoint.</i>
        Disjoint data means that each of the features present must be mutually exclusive from one another, and have no common elements between them.
        If the data is not disjoint, the tree will become circular and can, in effect, lead to infinit loops where no possible classification may result.
        In these instances where the data has common features are not mutually exclusive, it is best to remove or modify the samples in the data such that the tree can continue to run harmoniously.
        In the below code and example, all cases of non disjoint features have been removed entirely.
        Furthermore, a visual example of disjoint data is displayed below in its simplest, statistical definition (referenced from <a href = "https://www.math-only-math.com/disjoint-of-sets-using-Venn-diagram.html">MathOnlyMath</a>).
    </p>
    <h3 class = "center">Disjoint Data</h3>
    <img src = "./plots/disjoint_data.png" class = "center">

    <h1>Data Cleaning and Prep</h1>
    <p>
        The raw data for the following decision trees was as follows
    </p>
    <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }
        
            .dataframe tbody tr th {
                vertical-align: top;
            }
        
            .dataframe thead th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr style="text-align: right;">
              <th></th>
              <th>age</th>
              <th>job_industry_recode</th>
              <th>school_enroll_lastweek</th>
              <th>employment</th>
              <th>fam_size</th>
              <th>...</th>
              <th>fam_otc_med_costs</th>
              <th>fam_rent_val</th>
              <th>fips</th>
              <th>state</th>
              <th>state.1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th>1</th>
              <td>67</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>4</td>
              <td>23</td>
            </tr>
            <tr>
              <th>2</th>
              <td>74</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>60</td>
              <td>0</td>
              <td>ME</td>
              <td>4</td>
              <td>23</td>
            </tr>
            <tr>
              <th>3</th>
              <td>66</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>1</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
            <tr>
              <th>4</th>
              <td>68</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>1</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
            <tr>
              <th>5</th>
              <td>52</td>
              <td>Not in universe, or children</td>
              <td>No</td>
              <td>0</td>
              <td>1</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
          </tbody>
        </table>
        <p>5 rows × 38 columns</p>
        </div>
    <h3>Initial Data Cleaning</h3>
    <p>
        Decision trees are a powerful tool in partitioning data and are capable of reading both categorical and numeric data.
        However, in python, it must be noted that there must be extra special care taken when combining data types as python can and will inadvertently add extra weight to certain features based on how categorical data must be encoded.
        Thus, when selecting categorical columns, only those where the column could be drastically reduced or one hot encoded without drastically increasing dimensionality were used.
        Many of these columns were originally selected based on the multinomialnaive bayes and categorical naive bayes datasets.
        Refer to the Naive Bayes tab for further clarification.
        The following columns were selected for use in the decision trees: federal income, family size, age, family medical costs, weeks unemployed, hours worked, job industry, highest education, marital status, 
        full time or part time, job industry, sex, full time or part time school.
        Of these categories job industry, highest education, marital status, full time or part time, sex, and full time or part time school were all categorical variables that needed to be encoded.
        One hot encoding was selected, and while it increased dimensionality, many of the features were dummy features or had few categories such that it did not impact the data heavily.
        However, age, job industry, and education did have a drastic amount of categories and as such were remapped into larger groupings. 
        For example, management operations and administrative jobs were all remapped to be within a white collar job category.
        There was again an issue of balance as noted in the naive bayes section.
        To solve this, the training data was randomly undersampled to make a more balanced training dataset.
        This prevents artificially inflating accuracy scores and enables the trees to be more capable at identifying the minority classes.
        Lastly, in between each of the trees a feature was dropped to better understand the relationships in the data and provide unique root nodes.
        In the first decision tree, all features were used, in the second tree all but income were included in the data, adn in the third tree all but training, marital status, and age were used to create the tree.
        The overall final training and testing data is included below.
    </p>
    <h3>Training Data</h3>
    <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }
        
            .dataframe tbody tr th {
                vertical-align: top;
            }
        
            .dataframe thead th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr style="text-align: right;">
              <th></th>
              <th>federal_gross_income_adj</th>
              <th>fam_size</th>
              <th>age</th>
              <th>fam_med_costs</th>
              <th>weeks_unemployed</th>
              <th>...</th>
              <th>labor_force_recode_Unemp,on layoff</th>
              <th>labor_force_recode_W/job,not at work</th>
              <th>labor_force_recode_Working</th>
              <th>sex_Female</th>
              <th>sex_Male</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th>103888</th>
              <td>0</td>
              <td>1</td>
              <td>5</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>89741</th>
              <td>0</td>
              <td>1</td>
              <td>14</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>71415</th>
              <td>0</td>
              <td>3</td>
              <td>2</td>
              <td>500</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>114301</th>
              <td>0</td>
              <td>0</td>
              <td>69</td>
              <td>60</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>93094</th>
              <td>0</td>
              <td>1</td>
              <td>4</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>...</th>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
            </tr>
            <tr>
              <th>91830</th>
              <td>0</td>
              <td>0</td>
              <td>29</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>132499</th>
              <td>2801</td>
              <td>1</td>
              <td>52</td>
              <td>400</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>18297</th>
              <td>0</td>
              <td>0</td>
              <td>59</td>
              <td>350</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>142277</th>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>65234</th>
              <td>0</td>
              <td>1</td>
              <td>15</td>
              <td>40</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>17840 rows × 49 columns</p>
        </div>
    <p>and the target variable counts were:</p>
    poverty ratio <br>
100 - 124 PERCENT OF THE POVERTY LEVEL :   4460 <br>
125 - 149 PERCENT OF THE POVERTY LEVEL  :  4460<br>
150 AND ABOVE THE POVERTY LEVEL        :   4460<br>
BELOW POVERTY LEVEL                     :  4460<br>

    <h3>Testing Data</h3>
    <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }
        
            .dataframe tbody tr th {
                vertical-align: top;
            }
        
            .dataframe thead th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr style="text-align: right;">
              <th></th>
              <th>federal_gross_income_adj</th>
              <th>fam_size</th>
              <th>age</th>
              <th>fam_med_costs</th>
              <th>weeks_unemployed</th>
              <th>...</th>
              <th>labor_force_recode_Unemp,on layoff</th>
              <th>labor_force_recode_W/job,not at work</th>
              <th>labor_force_recode_Working</th>
              <th>sex_Female</th>
              <th>sex_Male</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th>60456</th>
              <td>0</td>
              <td>1</td>
              <td>11</td>
              <td>150</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>61253</th>
              <td>312940</td>
              <td>1</td>
              <td>38</td>
              <td>3500</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>40491</th>
              <td>115103</td>
              <td>1</td>
              <td>34</td>
              <td>805</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>113407</th>
              <td>44020</td>
              <td>0</td>
              <td>32</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>1</td>
            </tr>
            <tr>
              <th>79087</th>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>1000</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>...</th>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
            </tr>
            <tr>
              <th>15647</th>
              <td>50002</td>
              <td>2</td>
              <td>23</td>
              <td>300</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>59576</th>
              <td>0</td>
              <td>1</td>
              <td>45</td>
              <td>1270</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>14999</th>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>330</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>98278</th>
              <td>22000</td>
              <td>1</td>
              <td>35</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>23969</th>
              <td>0</td>
              <td>1</td>
              <td>52</td>
              <td>4300</td>
              <td>0</td>
              <td>...</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>1</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>29172 rows × 49 columns</p>
        </div>
        <p>and the target variable counts were as follows:</p>
        poverty ratio <br>
150 AND ABOVE THE POVERTY LEVEL       :    23509<br>
BELOW POVERTY LEVEL                    :    3346<br>
125 - 149 PERCENT OF THE POVERTY LEVEL  :   1217<br>
100 - 124 PERCENT OF THE POVERTY LEVEL   :  1100<br>

    <h1>Results</h1>
    <h3 class = "center">Net Decision Tree and Confusion Matrix - Figure 1</h3>
    <p>Tree</p>
    <iframe class = "pdf" src = "./plots/Decision_Tree1.pdf" width = "500" height = "800"></iframe>
    <p>Confusion Matrix 1</p>
    <img src = "./plots/Decision_TreeCM.png" class = "center">
    <p>The accuracy score for the original net decision tree was 69.2%</p>
    <br>
    <br>
    <br>
    <h3 class = "center">Decision Tree Income Removed and Confusion Matrix - Figure 2</h3>
    <p>Tree</p>
    <iframe class = "pdf" src = "./plots/Decision_Tree2.pdf" width = "500" height = "800"></iframe>
    <p>Confusion Matrix 2</p>
    <img src = "./plots/Decision_Tree2CM.png" class = "center">
    <p>The accuracy score for the second decision tree was 54.2%</p>
    <br>
    <br>
    <br>
    <h3 class = "center">Decision Tree Age and Separated Marital Status Removed and Confusion Matrix - Figure 3</h3>
    <p>Tree</p>
    <iframe class = "pdf" src = "./plots/Decision_Tree3.pdf" width = "500" height = "800"></iframe>
    <p>Confusion Matrix 3</p>
    <img src = "./plots/Decision_Tree3CM.png" class = "center">
    <p>The accuracy score for the third tree was: 53.6%</p>

    <h1>Conclusions</h1>
    <p>
        In this project, decision trees are used primarily as a supervised learning technique to calssify the levels of impoverishment and to identify what key features are needed to do so.
        Looking to each of the results of the individual decision trees, the preliminary decision tree performs best scoring nearly 70% successful at correctly identifying the level of impoverishment.
        The drastic drop off in accuracy shows how substantial income is to the level of poverty somebody experiences despite costs a family may have to endure.
        One other substantial thing to notice is the level of entropy in each of the trees.
        There is high entropy exhibitted in all of the trees.
        This manifests primarily as noise, but further suggests a need for more data around the participants of the census.
        Furthermore, the initial decision tree exhibits all expense based features in the tree despite others showing categorical features as significant.
        This demonstrates that despite potential indicators of income, it truly is the expenses and costs that help to determine impoverishment even if the individuals that typically experience poverty are unbalanced in general.
        Additionally, the first tree is the only one to have a single node with less than a 0.05 gini coefficient, the closest to pure exhibitted.
        This means that there is a group of features or questions that one may ask in the decision tree to indicate at what level of poverty one experiences.
        Said node is based in income primarily, then medical costs, then family size.
    </p>

    <p>
        The following two nodes exhibit higher levels of entropy, lower information gain between the levels and generally higher gini coefficients.
        This again highlights the importance of income, but after dropping the feature provides substantial insight into what other features may be indicative of income in the US.
        The second tree again exhibits a cost, but further looks into what type of employment a person exhibits, either full or part time.
        This begs the question and leads to the assumption that part time employment may potentially lead to higher rates of poverty in the US.
        Furthermore, the third decision tree looks to family size and whether or not a person was married and separated.
        These again suggest that there is a possibility that having larger families and being divorced in the US can lead to higher rates of poverty.
    </p>
    <p>
        Outside of the accuracy scores, the confusion matrices also provide some unique insight.
        The first tree was able to identify and classify poverty at a very successful rate. 
        However, looking to the confusion matrix, it was only able to do so for those who are not impoverished and those who are.
        It noticable fails at classiying those at risk of poverty as seen by the true classification rates for the two classes being nearly a third of the other two trees.
        Both of the succeeding trees performed far better at identifying those at risk of poverty, those who are between 100 and 150% of the poverty threshold.
        In such a case that identifying those individuals is important, it may be more beneficial to not use the first tree despite the higher accuracy score.
    </p>