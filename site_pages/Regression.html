<!DOCTYPE html>
<html>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<head>
    <style>
        body {
            background-color: #EAE7DC ;
            margin: 0px;
            font-family:'Arial', 'Helvetica', sans-serif;
            font-size: 18px;
        }

        .home-image {
            background-image: url(./site_pages/IMG_3777.jpg);
            background-repeat: no-repeat;
            background-attachment: fixed;
            background-position: center top;
            background-size: 100%;
            height: 400px;
        }

        .home-image-text {
            left:1%;
            top:27%;
            position: relative;
            color: #e85a4f;
        }

        .navbar {
            overflow: hidden;
            background-color: #D8C3A5;
        }

        .navbar a {
            float: left;
            color: #e85a4f;
            text-align: center;
            font-size: 18 px;
            padding: 20px 24px;
            text-decoration: none;
        }

        .navbar a.active{
            background-color: #e98074;
            color: #EAE7DC;
        }

        .navbar a:hover{
            background-color: #8E8D8A;
            color: #EAE7DC;
        }

        p {
            text-indent: 50px;
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        a {
            text-decoration: none;
        }    

        h1 {
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        h3 {
            line-height: 150%;
            text-align: justify;
            margin: 20px;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
        }

    </style>
</head>
<body>
    <div class = "navbar">
        <a href ="../index.html">Introduction</a>
        <a href = "./EDA.html">EDA</a>
        <a href = "./ARM.html">ARM</a>
        <a href = './Clustering.html'>Clustering</a>
        <a href = "./NB.html">Naive Bayes</a>
        <a href = "./PCA.html">PCA</a>
        <a class = "active" href = "./Regression.html">Regression</a>
        <a href = "./SVM.html">SVM</a>
        <a href = "./DT.html">DT</a>
        <a href = "./Conclusions.html">Conclusions</a>
    </div>

    <h1>Linear and Logistic Regression</h1>

    <p>
        Linear regression is often the first studied machine learning models in statistics where one or more features is used to describe their impact on a dependent or target variable.
        The final goal of linear regression is to find some sort of linear relationship between all of the variables and create a line (in simple linear regressions) or a plane (in mutliple linear regression) where each dimension has some predetermined weight that will impact the dependent variable.
        That is to say that an increase of one in a given dimension will increase or decrease the value of the dependent variable by the new found weight.
        Said weights are determined by minimizing the sum of the squared residuals across the domain of the problem, or, more specifically, by minimizing the difference of the actual value and their corresponding predicted value.
        Linear regression ultimately aims to predict the value of a target variable using the values given from every input.
    </p>

    <p>
        Logistic regression is another supervised learning algorithm used in prediction.
        It is used to classify binary outcomes based on an activation function called the sigmoid.
        In this, any value less than the threshold of the sigmoid is categorized as a 0 and anything greater is classified as a 1.
        Similar to linear regression multiple inputs and their weights are used to determine the value, however in this case, all raw scores are returned between 0 and 1 and are normalized.
        This effectively provides a probability of whether the outcome is in a particular class or not.
    </p>

    <p>
        The two models themselves are closely related to one another, and their differences nuanced.
        The most notable similarity being that the two models are used in supervised learning and require inputs that are quantitative and either binary (in the case of dummy variables) or continuous.
        While the two are both used in prediction, they do have different goals as linear regression returns a continuous output and gives an estimated value based on the inputs.
        Logistic regression on the other hand only ever returns a value between 0 and 1 and as such is used for binary categorization.
        Furthermore, unlike linear regression, logistic regression does not aim to minimize the error, rather it used maximum likelihood estimation to optimize (or maximize) the likelihood that the parameters take on given values.
    </p>

    <p>
        One particularly unique part of logistic regression is the use of the sigmoid function.
        The sigmoid function is the key or the link that joins linear regression to a probability output that is then used to categorize a binary outcome as a 0 or 1.
        The sigmoid itself is bounded between 0 and 1 as an output, while its inputs can exist from negative infinity to positive infinity.
        Furthermore, the sigmoid is smooth and symmetrical.
        An example of a sigmoid is demonstrated below alongside the equation for the sigmoid function and is referenced from <a href = "https://www.linkedin.com/pulse/logistic-regression-sigmoid-function-explained-plain-english-hsu/">LinkedIn</a>.
    </p>
    <h3 class = "center">The Sigmoid</h3>
    <img src = "./plots/Sigmoid full.jpg" class = "center">

    <p>
        In order to get the parameters (or the respective weights corresponding to a given input), maximum likelihood estimation (MLE) is used.
        MLE is often the better choice for non linear models (where linear models use minimizing the squares as in the case of OLS) as it is more generalized and often is more efficient at providing correct parameters.
        The likelihood estimation of the function represents the probability of observing the outputs given the parameters of the model.
        By maximizing the likelihood function, the optimal parameters yielded are effectively the most likely values for the weights that impact the dependent variable.
    </p>

    <h3>Data Prep and Code</h3>
    <p>
        Following suit with adjacent tabs, the main goal of these models is to classify people who are imoverished.
        However, since logistic regression requires a binary output, any target at or above 150 percent of the threshold for poverty was remapped to represent not impoverished persons.
        Each of the other three classes was remapped to impoverished to represent both those who are impoverished and those who are the closest to at risk of poverty.
        This leaves the data to have only 2 target labels as required for logistic regression.
        Immediately following this, feature variables were selected and include federal gross income, family size, age, family medical costs, time unemployesd, total hours worked in the week, sex, and whether they were employed or not.
        While there are other features that would be valuable (like job clasification), the use of mixed data (both categorical and numeric) in this case does not work for multinomial naive bayes in python and would make a logistic regression model too complex as there would be a need to dummify the data (the categorical variables are not inherently ordinal).
        As such, they were removed from the original census data, and only the numerical and dummy variables were included.
        Lastly, the issue of balance in the training data again remains meaning that the training data needed to be randomly undersampled in order to enable the models to learn to better classify the minority labels.
        This means that the overall data was split into two groups (training and testing) then the training data was resampled until equal values of each of the target variables was exhibitted.
        The raw data is displayed below followed by the final training and testing data to be used in both models.
    </p>

    <h3>Raw Data</h3>
    <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }
        
            .dataframe tbody tr th {
                vertical-align: top;
            }
        
            .dataframe thead th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr style="text-align: right;">
              <th></th>
              <th>age</th>
              <th>job_industry_recode</th>
              <th>school_enroll_lastweek</th>
              <th>employment</th>
              <th>fam_size</th>
              <th>full_time_work</th>
              <th>full_part_school</th>
              <th>highest_education</th>
              <th>hs_col_unenrolled</th>
              <th>weekly_earnings</th>
              <th>...</th>
              <th>poverty_ratio</th>
              <th>family_earn_ly</th>
              <th>fam_id</th>
              <th>fam_med_costs</th>
              <th>fam_outofpocket_med_costs</th>
              <th>fam_otc_med_costs</th>
              <th>fam_rent_val</th>
              <th>fips</th>
              <th>state</th>
              <th>state.1</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th>1</th>
              <td>67</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>Some College But No Degree</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>BELOW POVERTY LEVEL</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>4</td>
              <td>23</td>
            </tr>
            <tr>
              <th>2</th>
              <td>74</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>High school graduate-high school diploma</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>BELOW POVERTY LEVEL</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>60</td>
              <td>60</td>
              <td>0</td>
              <td>ME</td>
              <td>4</td>
              <td>23</td>
            </tr>
            <tr>
              <th>3</th>
              <td>66</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>High school graduate-high school diploma</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>150 AND ABOVE THE POVERTY LEVEL</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
            <tr>
              <th>4</th>
              <td>68</td>
              <td>Not in universe, or children</td>
              <td>Not in univ. or children &amp; Armed Forces</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>High school graduate-high school diploma</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>150 AND ABOVE THE POVERTY LEVEL</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
            <tr>
              <th>5</th>
              <td>52</td>
              <td>Not in universe, or children</td>
              <td>No</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
              <td>0</td>
              <td>High school graduate-high school diploma</td>
              <td>0</td>
              <td>0</td>
              <td>...</td>
              <td>150 AND ABOVE THE POVERTY LEVEL</td>
              <td>42000</td>
              <td>1</td>
              <td>50</td>
              <td>1370</td>
              <td>0</td>
              <td>0</td>
              <td>ME</td>
              <td>3</td>
              <td>23</td>
            </tr>
          </tbody>
        </table>
        <p>5 rows × 38 columns</p>
        </div>
    <h3>Training Data</h3>
    <p>
        Feature data is as follows
    </p>
    <div>
        <style scoped>
            .dataframe tbody tr th:only-of-type {
                vertical-align: middle;
            }
        
            .dataframe tbody tr th {
                vertical-align: top;
            }
        
            .dataframe thead th {
                text-align: right;
            }
        </style>
        <table border="1" class="dataframe">
          <thead>
            <tr style="text-align: right;">
              <th></th>
              <th>federal_gross_income_adj</th>
              <th>fam_size</th>
              <th>age</th>
              <th>fam_med_costs</th>
              <th>weeks_unemployed</th>
              <th>hours_worked</th>
              <th>sex</th>
              <th>employment</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <th>110777</th>
              <td>0</td>
              <td>1</td>
              <td>52</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <th>19343</th>
              <td>4044</td>
              <td>1</td>
              <td>80</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>8390</th>
              <td>0</td>
              <td>1</td>
              <td>48</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>122812</th>
              <td>0</td>
              <td>1</td>
              <td>14</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <th>78625</th>
              <td>0</td>
              <td>1</td>
              <td>57</td>
              <td>120</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>...</th>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
              <td>...</td>
            </tr>
            <tr>
              <th>51995</th>
              <td>0</td>
              <td>1</td>
              <td>11</td>
              <td>250</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <th>34409</th>
              <td>0</td>
              <td>1</td>
              <td>4</td>
              <td>5000</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <th>106055</th>
              <td>0</td>
              <td>1</td>
              <td>13</td>
              <td>1200</td>
              <td>0</td>
              <td>0</td>
              <td>1</td>
              <td>0</td>
            </tr>
            <tr>
              <th>145010</th>
              <td>0</td>
              <td>2</td>
              <td>12</td>
              <td>200</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
            <tr>
              <th>129535</th>
              <td>116827</td>
              <td>1</td>
              <td>59</td>
              <td>310</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
              <td>0</td>
            </tr>
          </tbody>
        </table>
        <p>44140 rows × 8 columns</p>
        </div>
    <p>
        The corresponding label counts are as follows
    </p>
    poverty ratio <br>
Impoverished    :    22070 <br>
Not Impoverished   : 22070<br>

<h3>Testing Data</h3>
<p>
    The Testing data is as follows
</p>
<div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }
    
        .dataframe tbody tr th {
            vertical-align: top;
        }
    
        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>federal_gross_income_adj</th>
          <th>fam_size</th>
          <th>age</th>
          <th>fam_med_costs</th>
          <th>weeks_unemployed</th>
          <th>hours_worked</th>
          <th>sex</th>
          <th>employment</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>8052</th>
          <td>0</td>
          <td>1</td>
          <td>54</td>
          <td>12000</td>
          <td>0</td>
          <td>10</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr>
          <th>21158</th>
          <td>0</td>
          <td>1</td>
          <td>51</td>
          <td>999</td>
          <td>0</td>
          <td>50</td>
          <td>0</td>
          <td>1</td>
        </tr>
        <tr>
          <th>69026</th>
          <td>4800</td>
          <td>0</td>
          <td>69</td>
          <td>200</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <th>81717</th>
          <td>160001</td>
          <td>1</td>
          <td>38</td>
          <td>0</td>
          <td>0</td>
          <td>72</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr>
          <th>101671</th>
          <td>0</td>
          <td>1</td>
          <td>17</td>
          <td>4000</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
          <td>0</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>112679</th>
          <td>0</td>
          <td>1</td>
          <td>40</td>
          <td>8500</td>
          <td>0</td>
          <td>15</td>
          <td>0</td>
          <td>1</td>
        </tr>
        <tr>
          <th>9847</th>
          <td>10002</td>
          <td>1</td>
          <td>20</td>
          <td>0</td>
          <td>0</td>
          <td>35</td>
          <td>0</td>
          <td>1</td>
        </tr>
        <tr>
          <th>139909</th>
          <td>91928</td>
          <td>1</td>
          <td>38</td>
          <td>200</td>
          <td>0</td>
          <td>2</td>
          <td>0</td>
          <td>1</td>
        </tr>
        <tr>
          <th>77000</th>
          <td>54605</td>
          <td>1</td>
          <td>47</td>
          <td>1400</td>
          <td>0</td>
          <td>25</td>
          <td>1</td>
          <td>1</td>
        </tr>
        <tr>
          <th>109282</th>
          <td>32538</td>
          <td>0</td>
          <td>64</td>
          <td>20000</td>
          <td>0</td>
          <td>25</td>
          <td>0</td>
          <td>1</td>
        </tr>
      </tbody>
    </table>
    <p>28702 rows × 8 columns</p>
    </div>
<p>
    And the target variable counts are as follows
</p>
poverty ratio <br>
Not Impoverished :   23146 <br>
Impoverished      :   5556 <br>

<h1>Results</h1>
<p>
    Below is included the results of each of the models to include the confusion matrices and the accuracy scores.
    The Logistic Regression model outperforms the Multinomial Naive Bayes by a substantial margin (nearly 15% in accuracy) and the confusion matrix appears to be much more balanced on the diagonals
</p>
<h3>Logistic Regression</h3>
<img src = "./plots/logregCM.png" class = "center">
<p>The accuracy score for the logistic regression was 64.5%</p>
<h3>Multinomial Naive Bayes</h3>
<img src = "./plots/Regression_MNBCM.png" class = "center">
<p>The accuracy score for the multinomial naive bayes was 49.4% (nearly 7% better than in the Naive Bayes section due to some assumptions and increased variables)</p>

<h1>Conclusions</h1>
<p>
    In this module, both logistic regression and multinomial naive bayes were used to classify the existence of poverty in the American people.
    Looking to the above accuracy scores and confusion matrices, the logistic regression far outperforms the multinomial naive bayes model.
    Based on the accuracy of around 50% alone, the naive bayes has, effectively, a random chance of selecting the correct classification.
    Comparatively, the logistic regression is roughly 15% better at classification.
    While the logistic regression scores are still low, it is still a substantially better model.
    This suggests 2 main things, that more daa that is uncorrelated is needed and that the bias that is exhibitted naturally in the naive bayes model is a hiuge detriment to its ability to classify.
    This likely comes from the necessity to assume independence of the variables when in real life, as seen above by raw census data, this is not always the case.
    There are many features in the model that may be highly correlated like age and income, as the older one gets, the more likely they are to make more money from both higher experience working and greater education.
    Further research and model development is needed and the results of this suggest that a different model may be needed that could learn from the mistakes of this one.
    One such example could be a ridge or lasso regression that is still linear in nature but penalizes the correlation by regularization.
    Looking to the data itself, aside from learning that there is high variance that combats the bias preference of the multinomial naive bayes that the logistic regression can better combat, the results suggest that impoverishment specifically is more difficult to predict.
    Based on the matrix alone, the model has more false positives for impoverishment than false positives in both cases.
    Ultimately this guides the need for data to provide information on impoverished people specifically rather than classifying people out of poverty.
</p>

